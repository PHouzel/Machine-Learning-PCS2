{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3bb582",
   "metadata": {},
   "source": [
    "# Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b95d4",
   "metadata": {},
   "source": [
    "### Import libraries and preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "415afc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nibabel\n",
    "#!{sys.executable} -m pip install nilearn #install relevant neuroimaging package\n",
    "#!{sys.executable} -m pip install opencv-python #install video package\n",
    "from preprocessing import load_and_mask_miyawaki_data\n",
    "from plots import plt_fmri_stim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "88c60682",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the masked data is (20, 145, 5438)\n",
      "Preprocessed fMRI data: 2860 samples x 5438 voxels\n",
      "Preprocessed stimuli data: 2860 samples x 100 pixels\n",
      "1536 geometrical figures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Done (23.55s).\n"
     ]
    }
   ],
   "source": [
    "fmri_data, stimuli, fmri_figures_data, stimuli_figures, masker = load_and_mask_miyawaki_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "102c36eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "       -1., -1., -1., -1., -1., -1., -1., -1., -1.])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "07566320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0.0, 1.0, 0.0]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[-1.,-1.,-1.],[0,1,0]])\n",
    "a = [[0 if k == -1 else k for k in abc] for abc in a]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98ebb7",
   "metadata": {},
   "source": [
    "### Create data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "22a925da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fmri = np.concatenate((fmri_data, fmri_figures_data))\n",
    "all_stimuli = np.concatenate((stimuli, stimuli_figures))\n",
    "\n",
    "#erase black figs: \"\"\"\"\"\"useless\"\"\"\"\"\" for our use of the data\n",
    "        \n",
    "stimuli2 = np.array([element for element in stimuli if np.sum(element) != -100])\n",
    "fmri_data2 = np.array([fmri_data[i] for i in range(len(fmri_data)) if np.sum(stimuli[i]) != -100])\n",
    "\n",
    "all_stimuli2 = np.array([element for element in all_stimuli if np.sum(element) != -100])\n",
    "all_fmri2 = np.array([all_fmri[i] for i in range(len(all_fmri)) if np.sum(all_stimuli[i]) != -100])\n",
    "\n",
    "stimuli_figures2 = np.concatenate((np.array([element for element in stimuli_figures if np.sum(element) != -100]),np.array([stimuli_figures[4]])))\n",
    "fmri_figures_data2 = np.concatenate((np.array([fmri_figures_data[i] for i in range(len(fmri_figures_data)) if np.sum(stimuli_figures[i]) != -100]), np.array([fmri_figures_data[4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "99b472b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#random images avec les carrés noirs\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(fmri_data, stimuli, \n",
    "                                        train_size=train_to_test_ratio, \n",
    "                                        test_size=1.0-train_to_test_ratio)\n",
    "\n",
    "#geometric images avec les carrés noirs\n",
    "train_to_test_ratio = 0.8\n",
    "X_trainfig, X_testfig, Y_trainfig, Y_testfig =train_test_split(fmri_figures_data, stimuli_figures, \n",
    "                                        train_size=train_to_test_ratio, \n",
    "                                        test_size=1.0-train_to_test_ratio)\n",
    "\n",
    "#geometric images sans les carrés noirs\n",
    "train_to_test_ratio = 0.8\n",
    "X_trainfig2, X_testfig2, Y_trainfig2, Y_testfig2 =train_test_split(fmri_figures_data2, stimuli_figures2, \n",
    "                                        train_size=train_to_test_ratio, \n",
    "                                        test_size=1.0-train_to_test_ratio)\n",
    "\n",
    "#random images sans les carrés noirs\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(fmri_data2, stimuli2, \n",
    "                                        train_size=train_to_test_ratio, \n",
    "                                        test_size=1.0-train_to_test_ratio)\n",
    "\n",
    "#avec tout\n",
    "X_train_all, X_test_all, Y_train_all, Y_test_all =train_test_split(all_fmri, all_stimuli, \n",
    "                                        train_size=train_to_test_ratio, \n",
    "                                        test_size=1.0-train_to_test_ratio)\n",
    "\n",
    "X_train_all2, X_test_all2, Y_train_all2, Y_test_all2 =train_test_split(all_fmri2, all_stimuli2, train_size=train_to_test_ratio, test_size=1.0-train_to_test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82acf125",
   "metadata": {},
   "source": [
    "### Manipulate stimuli images for multiscale analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4515901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list pixel_indices of 4-uplets of incidices of pixels we want to average together for the 2*2 scale\n",
    "k = [2*i for i in range(5)] #list used to generate pixel_indices\n",
    "pixel_indices_22 = [[10*a+b, 10*a+(b+1), 10*(a+1)+b, 10*(a+1)+(b+1)] for a in k for b in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ef6d9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of stimuli data for multi scale encoding\n",
    "\n",
    "def preproc_one_stimuli(idx_pxl_toavg, stimuli, reshape = False, reshape_form = (5,5)) :\n",
    "    \"\"\"\n",
    "    Permits to preproc 1 stimuli figure by averaging some pixels together for the \n",
    "    multiscale encoding.\n",
    "    - idx_pxl_toavg : liste de multiplets d'indices de pixels que l'on souhaite moyenner\n",
    "                      ensemble. Par ex, pour le 2*2 on a :\n",
    "                      [[0,1,10,11], [2,3,12,13], ... ,[88,89,98,99]].\n",
    "    - stimuli: the image\n",
    "    - reshape : if you want your image to be reshaped in such a way that you can plot it\n",
    "    - reshape_form : (5,5) for a 5 by 5 image, etc...\n",
    "    \"\"\"\n",
    "    if reshape == False :\n",
    "        return np.array([np.average([stimuli[i] for i in idx]) for idx in idx_pxl_toavg])\n",
    "    \n",
    "    else :\n",
    "        return np.array(np.reshape([np.average([stimuli[i] for i in idx]) for idx in idx_pxl_toavg],(5,5)))\n",
    "    \n",
    "\n",
    "def preproc_multiple_stimuli(idx_pxl_toavg, stimuli_list, reshape = False, reshape_form = (5,5)) :\n",
    "    \"\"\"\n",
    "    Same as above but to preproc a whole list of stimulis\n",
    "    Sorry this looks horrible\n",
    "    \"\"\"\n",
    "    if reshape == False :\n",
    "        return np.array([[np.average([stimuli[i] for i in idx]) for idx in idx_pxl_toavg] for stimuli in stimuli_list])\n",
    "    \n",
    "    else :\n",
    "        reshape_form_multi = (len(stimuli_list),reshape_form[0],reshape_form[1])\n",
    "        return np.array(np.reshape([np.average([stimuli[i] for i in idx]) for idx in idx_pxl_toavg for stimuli in stimuli_list],reshape_form_multi))\n",
    "    \n",
    "def Y_train_newscale(Y_train, idx_pxl_toavg) :\n",
    "    \"\"\"\n",
    "    Creates a new Y_train but with rescaled images \n",
    "    Changes greyscale values to classes (integers) : for this, we want to map the values of greyscale to\n",
    "    integers. We notice that for 4 pixels, only possible values are [0,0.25,0.5,0.75,1]. For 2 pixels, [0,0.5,1]. Etc.\n",
    "    By multiplying these grayscale values by the number of pixels we get [0,1,2,3,4] or [0,1,2], etc. Hence the multiplication.\n",
    "    \"\"\"\n",
    "    Y_continuous = preproc_multiple_stimuli(idx_pxl_toavg, Y_train) #with values in [0,1]\n",
    "    n = len(idx_pxl_toavg[0]) #number of classes is the number of pixels there is to average + 1. For instance for 4 pixels you can have [0,0.25,0.5,0.75,1]\n",
    "    Y_integers = [[int(pixel_val * n) if pixel_val != -1 else 0 for pixel_val in img] for img in Y_continuous]\n",
    "    return Y_integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "734135dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1892490dc0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that stimuli processing works\n",
    "\n",
    "imag = stimuli_figures2[30]\n",
    "\n",
    "old = np.reshape(imag,(10,10))\n",
    "new = preproc_one_stimuli(pixel_indices_22, imag, reshape = True)\n",
    "\n",
    "fig = plt.figure()\n",
    "sp1 = plt.subplot(121)\n",
    "sp2 = plt.subplot(122)\n",
    "\n",
    "# to show the grid \n",
    "\n",
    "for ax in (sp1, sp2) :\n",
    "\n",
    "    # Major ticks\n",
    "    ax.set_xticks(np.arange(0, 10, 1))\n",
    "    ax.set_yticks(np.arange(0, 10, 1))\n",
    "\n",
    "    # Minor ticks\n",
    "    ax.set_xticks(np.arange(-.5, 10, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, 10, 1), minor=True)\n",
    "    \n",
    "    ax.grid(which='minor', color='r', linestyle='-', linewidth=2)\n",
    "\n",
    "\n",
    "sp1.imshow(old, cmap = plt.cm.gray, interpolation = 'nearest')\n",
    "sp2.imshow(new, cmap = plt.cm.gray, interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65d470",
   "metadata": {},
   "source": [
    "It works !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2ddf2",
   "metadata": {},
   "source": [
    "# Train a log reg at the different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0a524523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def train_logreg_scale(X_train, Y_train, X_test, Y_test, scale) :\n",
    "    \"\"\"\n",
    "    Trains a log reg for a certain scale \n",
    "    \"\"\"\n",
    "    logreg_table = []\n",
    "    size = int(100/scale[0]*scale[1])\n",
    "    \n",
    "    train_accuracy, test_accuracy = np.zeros(size), np.zeros(size)\n",
    "\n",
    "    # loop over regularisation strength\n",
    "    for i, pixel in tqdm(enumerate(np.transpose(Y_train))):\n",
    "    \n",
    "        # define logistic regressor\n",
    "        logreg = linear_model.LogisticRegression(penalty=\"l2\", C=0.05, max_iter = 15000, multi_class='multinomial')\n",
    "    \n",
    "        # fit training data\n",
    "        logreg.fit(X_train, pixel)\n",
    "    \n",
    "        logreg_table.append(logreg)\n",
    "    \n",
    "        # check accuracy\n",
    "        train_accuracy[i]=logreg.score(X_train,pixel)\n",
    "        test_accuracy[i]=logreg.score(X_test,np.transpose(Y_test)[i])\n",
    "        \n",
    "    return logreg_table, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "777340e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [16:20,  9.81s/it]\n"
     ]
    }
   ],
   "source": [
    "#Prediction functions for initial pixels\n",
    "[logreg_table_11, train_accuracy_11, test_accuracy_11] = train_logreg_scale(X_train, Y_train, X_test, Y_test, (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a342d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Y_train for 2*2 block averaged images\n",
    "Y_train_22, Y_test_22 = Y_train_newscale(Y_train, pixel_indices_22), Y_train_newscale(Y_test, pixel_indices_22)\n",
    "\n",
    "[logreg_table_22, train_accuracy_22, test_accuracy_22] = train_logreg_scale(X_train, Y_train_22, X_test, Y_test_22,(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d5fb1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1.]),\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 47\n",
    "Y_train[n], Y_train_22[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "18892cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=48\n",
    "imag = stimuli_figures2[n]\n",
    "\n",
    "old = np.reshape(imag,(10,10))\n",
    "new = preproc_one_stimuli(pixel_indices_22, imag, reshape = True)\n",
    "pred = np.zeros(25)\n",
    "\n",
    "xtest = fmri_figures_data2[n].reshape(1,-1)\n",
    "ytest = new\n",
    "\n",
    "for i,pixel in enumerate(pred) :\n",
    "    y = logreg_table_22[i].predict(xtest)\n",
    "    \n",
    "    pred[i] = y\n",
    "    \n",
    "pred = np.reshape(pred,(5,5))\n",
    "\n",
    "pred_greyscale = [x/4 for x in pred]\n",
    "\n",
    "\n",
    "def predict_stim(logreg_table, X, size):\n",
    "    \n",
    "    pred = np.zeros(size)\n",
    "\n",
    "    for i,pixel in enumerate(pred) :\n",
    "        \n",
    "        y = logreg_table_22[i].predict(xtest)\n",
    "        pred[i] = y\n",
    "        \n",
    "    pred = np.reshape(pred,(5,5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0c0f54ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7437062937062937"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "sp1 = plt.subplot(121)\n",
    "sp2 = plt.subplot(122)\n",
    "sp1.imshow(ytest, cmap = plt.cm.gray, interpolation = 'nearest')\n",
    "sp2.imshow(pred_greyscale, cmap = plt.cm.gray, interpolation = 'nearest')\n",
    "np.average(test_accuracy_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63dc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all multiscale predictions\n",
    "\n",
    "fig = plt.figure()\n",
    "sp1 = plt.subplot(121)\n",
    "sp2 = plt.subplot(122)\n",
    "sp1.imshow(ytest, cmap = plt.cm.gray, interpolation = 'nearest')\n",
    "sp2.imshow(pred_greyscale, cmap = plt.cm.gray, interpolation = 'nearest')\n",
    "sp2.imshow(pred_greyscale, cmap = plt.cm.gray, interpolation = 'nearest')\n",
    "np.average(test_accuracy_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c98749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
